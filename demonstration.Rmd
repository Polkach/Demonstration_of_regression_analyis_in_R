---
title: "Демонстрация регрессионного анализа в R"
author: "Комков Степан"
date: '5 марта 2017 г '
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE)
```
## Данные
Для начала подключим пакет, содержащий данные, а так же все пакеты, которые понадобятся нам при работе:
```{r,include=FALSE}
library("knitr")
library("pander")
```{r}
library("ggplot2")
library("dplyr")
library("dummies")
library("caret")
library("car")
library("lmtest")
library("plm")
library("ModelMetrics")
library("erer")
library("AUC")
library("memisc")
```
```{r,include=FALSE}
db = diamonds
db <- db[sample(nrow(db)),]
```
Исследовать будем базу по проданным бриллиантам из пакета ggplot2. Посмотрим на типы данных в базе:
```{r}
str(db)
```
price - цена бриллианта в долларах, carat - вес, cut - качество огранки, color - цвет, clarity - прозрачность, x - длина, y - ширина, z - глубина, depth - процент глубины алмаза, table - ширина верхней поверхности относительно наибольшей ширины. Посмотрим гистограмму цен проданных бриллиантов:
```{r}
hist(db$price,xlab="Цена",ylab="Количество",main="Гистограмма цен на бриллианты")
```

Посмотрим на плотности распределения цен в зависимости от цвета бриллианта:
```{r}
ggplot(db,aes(color,price,fill=color))+geom_violin()+labs(x="Цвет",y="Цена")+guides(fill=FALSE)
```

Плотности для цветов D и E визуально похожи. Давайте проверим гипотезу о равенстве средних в этих двух группах:
```{r,include=F}
db1 = subset(db,color %in% c("D","E"))
```
```{r}
summary(aov(price~color,db1))
```
Итак, на уровне значимости в 5% мы не отвергаем гипотезу о том, что средние цены бриллиантов цвета D и E различаются. Посмотрим теперь на распределение цен в зависимости от огранки бриллианта:
```{r}
ggplot(db,aes(cut,price,color=cut))+geom_boxplot()+labs(x="Огранка",y="Цена")+guides(color=F)
```

По полученным графикам видно, что разброс цен у очень хороших бриллиантов больше, чем у просто хороших. Но давайте проверим гипотезу, что на самом деле дисперсии у этих двух типов бриллиантов равны:
```{r,include=FALSE}
db1 = subset(db,cut %in% c("Very Good","Good"))
```
```{r}
bartlett.test(data=db1,price~cut)
```
Наша гипотеза о равенстве дисперсий отвергается.

## Выбор линейной модели

```{r,include=FALSE}
db <- cbind(db,dummy(db$color)[,1:6])
db <- cbind(db,dummy(db$cut)[,1:4])
db <- cbind(db,dummy(db$clarity)[,1:7])
db <- db[,-c(2,3,4)]

names(db) <- c("carat","depth","table","price","x","y","z","colorD","colorE","colorF","colorG",
               "colorH","colorI","cutFair","cutGood","cutVeryGood","cutPremium","clarityI1",
               "claritySI2","claritySI1","clarityVS2","clarityVS1","clarityVVS2","clarityVVS1")

in_train <- createDataPartition(y = db$price, p = 0.75, list = FALSE)
train <- db[in_train,]
test <- db[-in_train,]

model <- lm(price ~ ., train)
```
Перейдем к построению линейных моделей. Для этого будем рассматривать категориальные переменные (хоть они и упорядоченные) как дамми-переменные (набор переменных-индикаторов). А так же разобьем нашу выборку на тренировочную и тестовую в отношении 3 к 1. Построим линейную модель, объясняющую зависимую переменную цены через все остальные переменные. Исследуем получившуюся модель на значимость в целом:
```{r}
summary(model)
```
Как видим, модель объясняет более 90% дисперсии в данных. Все регрессоры, за исключением двух, статистически значимы. А так же сама регрессия значима в целом.
Как мы помним, средние цены в группах бриллиантов цветов D и E мы считаем равными. Проверим гипотезу о том, что переменные-индикаторы этих групп входят в модель с одинаковым весом:
```{r}
linearHypothesis(model,"colorD-colorE=0")
```
Наша гипотеза отвергается на любом разумном уровне значимости, а значит рассмотренные переменные вносят разный вклад в цену. Посмотрим новую модель, добавив комбинации признаков. С помощью них мы узнаем, как влияет цвет, качество и прозрачность на стоимость каждого нового грамма бриллианта, а так же нового сантиметра в каждом из трех измерений:
```{r,include=FALSE}
model2 <- lm(price ~ (.-carat-depth-table-x-y-z)*(carat+x+y+z)+depth+table, train)
```
```{r}
summary(model2)
```
Как мы видим, модель стала объяснять большую часть дисперсии, но теперь много предикторов статистически незначимы. Давайте сраним две имеющиеся модели:
```{r}
mtable(model, model2)$summaries
```
Хоть во второй модели значительно больше регрессоров, штрафные значения Акаике и Шварца у нее меньше. Давайте проверим гипотезу о том, что все добавленные регрессоры на самом деле лишние с помощью теста Вальда:
```{r}
waldtest(model,model2)
```
Как показывает пи-значение, гипотеза отвергается на любом разумном уровне значимости. Чтобы хоть как-то уменьшить количество переменных во второй модели, построим новую модель, пошагово убирая предикторы, руководствуясь значением Акаике. Посмотрим на оставшиеся регрессоры:
```{r}
step_model <- step(model2,direction="backward",trace=0)
formula(step_model)
```
## Исследование модели
При построении моделей оценки одних и тех же коэффициентов сильно менялись. Скорее всего среди наших признаков имеется сильная мультиколлениарность. Посмотрим на коэффициенты вздутия дисперсии:
```{r}
vif(step_model)
```
Как мы видим, у множества признаков рассматриваемые коэффициенты значительно превосходят 1000, что говорит о сильной мультиколлинеарности наших предикторов.
Однако отсутствие мультиколлинеарности не является для нас критичным свойством. Для эффективности оценок модели нужно, чтобы выполнялись следующие свойства для ошибок: условная гомоскедостичность, условная некоррелируемость и строгая экзогенность. Давайте посмотрим, соответствует ли модель этим предпосылкам. Для наблюдения условной гетероскедастичности, если она имеется, построим график остатков модели в зависимости от веса бриллианта:
```{r,include=FALSE}
train <- cbind(train,step_model$residuals)
train <- cbind(train,step_model$fitted.values)
```
```{r}
ggplot(aes(x=carat,y=step_model$residuals),data=train)+geom_point()
```

Как видно из графика в данных имеется явная условная гетероскедастичность. Чем больше бриллиант - тем больше варьируется его цена. Убедимся в этом, проведя тест Голдфелда—Куандта, сортируя наблюдения по весу, убирая средние 20% наблюдений:
```{r}
gqtest(step_model,order.by = ~carat, data=train, fraction = 0.2)
```
Как мы убедились, гипотеза о гомоскедастичности отвергается на любом разумном уровне значимости.
К счастью, наши данные не имеют никакой временной или географической структуры, так что мы можем считать, что в данных нет автокорреляции. Чтобы подтвердить это, проведем тесты Дарбина-Уотсона и Бройша—Годфри второго порядка:
```{r}
dwt(step_model)
bgtest(step_model,order=2)
```
Как видим, оба теста не отвергают нашу гипотезу даже на высоких уровнях значимости.
Итак, в наших данный присутствует гетероскедастичноть, но отсутствует автокорреляция. Значит для оценки ковариационной матрицы регрессоров достаточно использовать робастную оценку устойчивую к гетероскедастичности. Посмотрим, какие регрессоры значимы при таком оценивании:
```{r}
coeftest(step_model, vcov.=vcovHC(step_model))
```
По крайней мере вес бриллианта остается статистически значимым, что соответствует простой логике.
Одной из предпосылок эндогенности является пропуск регрессора. Наличие пропущенных регрессоров мы можем выявить с помощью теста Рамсея:
```{r}
resettest(step_model)
```
Гипотеза о наличии всех необходимых регрессоров отвергается на любом разумном уровне значимости. А значит, скорее всего, в наших данных присутствует эндогенность. Таким образом, наши оценки неэффективны, однако это не является помехой для использования модели в качестве предсказывающего алгоритма.

## Предсказания

Предскажем стоимость каждого бриллианта из тестовой выборки с помощью всех трех построенных моделей. Посмотрим на корень из среднеквадратичной ошибки для каждой модели:
```{r,include=FALSE}
step_rez <- predict(step_model,newdata = test)
original_rez <- predict(model,newdata = test)
poly_rez <- predict(model2,newdata = test)
```
```{r}
mse(test$price,original_rez)**0.5
mse(test$price,poly_rez)**0.5
mse(test$price,step_rez)**0.5
```
Наша финальная модель показывает результат схожий со второй моделью, но явно лучше чем у оригинальной модели.

## Логистическая регрессия

Воспользуемся данными по выборам президента США 96-го года.
```{r,include=FALSE}
db <- read.csv("elections_usa_96.csv")
db$vote <- factor(db$vote,labels=c(0,1))
in_train <- createDataPartition(y = db$vote, p = 0.75, list = FALSE)
train <- db[in_train,]
test <- db[-in_train,]
logit_model <- glm(data=train,vote~.,family=binomial(link="logit"))
```
```{r}
str(db)
```
popul - популяция в городе респондента, TVnews - количество дней недели, в которые респондент смотрит новости, ClinLR и DoleLR - насколько либерально или консервативно оценивает респондент программу соответствующего кандидата в президенты, age - возраст, educ - уровень образования (от неоконченного среднего до высшего специального), income - заработок, vote - голос. Разделим снова все наблюдения на тренировочную и тестовую выборки в отношении 3 к 1 и построим логистическую регрессию для предсказания голоса респондента на выборах:
```{r}
summary(logit_model)
```
Итак, уровень образования и просмотр новостей не являются статистически значимыми. Построим логистическую регрессию без этих предикторов:
```{r,include=FALSE}
logit_model2 <- glm(data=train,vote~.-educ-TVnews,family=binomial(link="logit"),x=T)
```
```{r}
summary(logit_model2)
```
Проверим гипотезу о том, что наши ограничения верны:
```{r}
lrtest(logit_model,logit_model2)
```
Как видно, гипотеза не отвергается даже на высоких уровнях значимости.
Давайте посмотрим, как выглядит средний респондент:
```{r}
summary(train)[4,]
```
И посмотрим на предельные эффекты признаков для этого респондента:
```{r}
maBina(logit_model2,x.mean=T)
```
Наконец, предскажем голоса избирателей из тестовой выборки. Построим для полученных значений ROC кривую:
```{r,include=FALSE}
pred <- predict(logit_model2,newdata=test,type="response")
roc.data <- roc(pred, test$vote)
```
```{r}
qplot(x = roc.data$fpr, y = roc.data$tpr, geom = "line")+xlab('FPR')+ylab('TPR')
```

Площадь под ROC кривой:
```{r}
auc(roc.data)
```
А так же для различных порогов выведем процент совпадения предсказанных голосов с настоящими голосами:
```{r}
for(i in seq(0.1,0.9,0.05)){
  pred_vote <- as.numeric(pred>i)
  cat(i,sum(pred_vote==test$vote)/nrow(test),'\n')
}
```